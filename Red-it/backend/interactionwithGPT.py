from openai import OpenAI
import tiktoken



def tokens_test(input_text):
    max_tokens = 16300                #Max tokens that can be generated by gpt3. You can change it according to the gpt model used
    splitted_words_limit = 16000
    tokens = input_text.split()
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo-0125")     #Encoding the tokens according to the model used.
    encoded_tokens = encoding.encode(" ".join(tokens))
    i = 1
    while len(encoded_tokens) > max_tokens: #Chopping given text until the tokens generated are less than 16300, which is the limit of gpt3
        if i == 1 and len(tokens) >= splitted_words_limit: #16000 is the token limit of gpt3 
            tokens = tokens[:splitted_words_limit]
            i += 1
        else:
            tokens = tokens[:len(tokens) - 500]
        encoded_tokens = encoding.encode(" ".join(tokens))
    return " ".join(tokens)

def generate_summary(data):
    data = tokens_test(data)
    client = OpenAI(api_key='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')
    response = client.chat.completions.create(
        model="gpt-3.5-turbo-0125",
        messages=[
            {"role": "system", "content": "You are an assistant skilled in summarizing texts, making complex information concise and straightforward."},
            {"role": "user", "content": f"Summarize this text: {data}"}
        ]
    )
    summary = response.choices[0].message.content
    summary = summary.strip()
    return summary
    
def remove_special_characters(string):
    string = string.replace('*', '')
    string = string.replace('-', '')
    return string

def generate_flashcard(data): # Function to generate flashcards from the input data. It retunrs a list of parsed flashcards
    data = tokens_test(data)
    client = OpenAI(api_key='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')
    response = client.chat.completions.create(
        model="gpt-3.5-turbo-0125",
        messages=[
            {"role": "system", "content": "You are an assistant skilled in generating flashcards from texts. Create flashcards that identify key points and their explanations."},
            {"role": "user", "content": f"Generate flashcards from this text: {data} Each flashcard must start with a number (e.g., '1.') and contain key points only, formatted in a single paragraph. Number of characters in each flashcard must be same. Generate a maximum of 10 flashcards."}
        ]
    )
    flashcards = response.choices[0].message.content
    flashcards = flashcards.strip()
    flashcards = flashcards.split("\n")
    parsed_flashcards = []
    for key_point in flashcards:
        if len(key_point) > 3:
            splitted_key_point = key_point.split(" ")
            if len(splitted_key_point) > 1:
                entity = " ".join(splitted_key_point[1:])
                entity = entity.replace("*", "")
                parsed_flashcards.append(entity)
    if len(parsed_flashcards) > 10 and len(parsed_flashcards)%2 == 0:
        updated_flashcards = []
        for i in range(0, len(parsed_flashcards), 2):
            updated_flashcards.append(remove_special_characters(parsed_flashcards[i]) + ": " + parsed_flashcards[i+1])
        return updated_flashcards
    return parsed_flashcards

def generate_powerpoint(data):
    # Implementing a logic similar to generate_summary for PowerPoint content
    data = tokens_test(data)
    client = OpenAI(api_key='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')
    response = client.chat.completions.create(
            model="gpt-3.5-turbo-0125",
            messages=[
                {"role": "system", "content": "You are an assistant skilled in generating presentation content from texts."},
                {"role": "user", "content": f"Produce presentation slides from the following text: {data}. For each slide, craft content that highlights key points along with their explanations. Begin every slide with a numeral (e.g., '1.'), followed by a slide title, and then the slide's content, all formatted within a single paragraph. Number of characters in each slide must be same. Number of characters in each title must be same. Generate a maximum of 10 slides."}
            ]
        )
    presentation = response.choices[0].message.content
    presentation = presentation.strip()
    slides = presentation.split("\n")
    parsed_slides = []
    for each_slide in slides:
        if len(each_slide) > 3:
            splitted_slide = each_slide.split(" ")
            if len(splitted_slide) > 1:
                parsed_slides.append(" ".join(splitted_slide[1:]))
    length = len(parsed_slides)
    if length > 10 and length%2 ==0:
        updated_slides = []
        for i in range(0, length, 2):
            updated_slides.append(remove_special_characters(parsed_slides[i]) + ": " + parsed_slides[i+1])
        return updated_slides
    return parsed_slides


def tokens_test_for_context_query(webpage_data, highlighted_text):
    max_tokens = 14300                #Max tokens that can be generated by gpt3 are 16300. We want tokens of the webpage content to be <=14000 so that the remainging are used for the highlighted text. You can change it according to the gpt model used
    splitted_words_limit = 14000
    tokens = webpage_data.split()
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo-0125")     #Encoding the tokens according to the model used.
    encoded_tokens = encoding.encode(" ".join(tokens))
    i = 1
    while len(encoded_tokens) > max_tokens: #Chopping given text until the tokens generated are less than 14300
        if i == 1 and len(tokens) >= splitted_words_limit: 
            tokens = tokens[:splitted_words_limit]
            i += 1
        else:
            tokens = tokens[:len(tokens) - 500]
        encoded_tokens = encoding.encode(" ".join(tokens))
    webpage_data_tokens = " ".join(tokens)
    max_tokens = 2000                #Remainging tokens for the highlighted text
    splitted_words_limit = 2000
    tokens = highlighted_text.split()
    encoded_tokens = encoding.encode(" ".join(tokens))
    i = 1
    while len(encoded_tokens) > max_tokens: #Chopping given text until the tokens generated are less than 14300
        if i == 1 and len(tokens) >= splitted_words_limit: 
            tokens = tokens[:splitted_words_limit]
            i += 1
        else:
            tokens = tokens[:len(tokens) - 500]
        encoded_tokens = encoding.encode(" ".join(tokens))
    highlighted_text_tokens = " ".join(tokens)
    return webpage_data_tokens, highlighted_text_tokens



def generate_context_query(webpage_data, highlighted_text):
    # Combine the webpage data and highlighted query with a suitable prompt
    webpage_data, highlighted_text = tokens_test_for_context_query(webpage_data, highlighted_text)
    client = OpenAI(api_key='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')
    response = client.chat.completions.create(
            model="gpt-3.5-turbo-0125",
            messages=[
                {"role": "system", "content": "You are an assistant skilled in interpreting and explaining texts."},
                {"role": "user", "content": f"Given the following text: {webpage_data}. Please explain the meaning of '{highlighted_text}' in the context of the entire data. Provide your explanation in a single paragraph format, without using any headings or titles. Keep it short and generate 1-2 sentence(s) only. Don't mention the highlighted query in your response; just give the explanation."}
            ],
            max_tokens=60 

        )
    explanation = response.choices[0].message.content
    explanation = explanation.strip()
    return explanation
